= Module 3 Lab

== Instructions

In this lab, we will use the features generated in the previous lab along with the phoneme state alignments provided in the course materials to train two different neural network acoustic models, a DNN and an RNN.

The inputs to the training program are:

* **lists/feat_train.rscp**, **lists/feat_dev.rscp** - List of training and dev feature files, stored in a format called RSCP.
This standard for relative SCP file, where SCP is HTK-shorthand for script file.
It is simply a list of files in the two sets.
The dev set is used in training to monitor overfitting and perform early stopping.
These files should have been generated as part of completing lab 2.
* **am/feat_mean.ascii**, **am/feat_invstddev.ascii** - The global mean and precision (inverse standard deviation) of the training features, also computed in lab 2
* **am/labels_all.cimlf**- The phoneme-state alignments that have been generated as a result of forced alignment of the data to an initial acoustic model.
Generating this file requires the construction of a GMM-HMM acoustic model which is outside the scope of this course, so we are providing it to you.
The labels for both the training and dev data are in this file.
* **am/labels.ciphones**- The list of phoneme state symbols which correspond to the output labels of the neural network acoustic model
* **am/ abels_ciprior.ascii** - The prior probabilities of the phoneme state symbols, obtained by counting the occurences of these labels in the training data.

The training, dev, and test RSCP files and the training set global mean and precision files were generated by the lab in Module 2.
The remaining files have been provided for you and are in the am directory.
=== Part 1: Training a feedforward DNN

We have provided a python program called **M3_Train_AM.py** which will train a feed-forward deep network acoustic model using the files described above.
The program is currently configured to train a network with the following hyperparameters:

* 4 hidden layers of size 512 hidden units per layer.
* 120 output units corresponding to the phoneme states
* input context window of 23 frames, which means the input to the network for a given frame is the current frame plus 11 frames in the past and 11 frames in the future
* minibatch size of 256
* Learning is performed with Momentum SGD with a learning rate of 1e-04 per sample with momentum as a time constant of 2500
* One epoch is defined as a complete pass of the training data and training will run for 100 epochs
* The development set will be evaluated every 5 epochs.

This can be executed by running

**$ python M3_Train_AM.py --type dnn**

On a GTX 965M GPU running on a laptop, the network trained as a rate of 63,000 samples/sec or about 20 seconds per epoch.
Thus 100 epochs will run in 2000 seconds or about 30 minutes.

After 100 epochs, the result of training, obtained from the end of the log file, was

**Finished Epoch[100 of 100]: [CE_Training] loss = 1.036854 * 1257104, metric = 32.74% * 1257104 17.146s (73317.6 samples/s);**

**Finished Evaluation [20]: Minibatch[1-11573]: metric = 44.26% * 370331;**

Thus, the training set has a cross entropy of 1.04 per sample, and a 32.74% frame error rate, while the held-out dev set has a frame error rate of 44.3%

After training is complete, you can visualize the training progress using **M3_Plot_Training.py**.
It takes a CNTK log file as input and will plot epoch vs. cross-entropy of the training set on one figure and epoch vs. frame error rate of the training and development sets on another figure.

**$ python M3_Plot_Training.py -–log <logfile>**

For this experiment, **<logfile>** would be **../am/dnn/log**

Here is an example of the figure produced by this script.

As you can see from the figure, overfitting has not yet occurred as the development set performance is still the best in the final epoch.
It is possible that small additional improvements can be obtained with additional training iterations.

You can now experiment with this neural network training script.
You can modify the various hyperparameters to see if the performance can be further improved.
For example, you can vary the

* Number of layers
* Number of hidden units in each layer
* Learning rate
* Minibatch size
* Number of epochs
* Learning algorithm (see the CNTK documentation for details on using other [learners](https://cntk.ai/pythondocs/cntk.learners.html), such as Adam or AdaGrad)

=== Part 2: Training a Recurrent Neural Network

In the second part of this lab, you will modify the code to train a Bidirectional LSTM (BLSTM) network, a type of recurrent neural network.

To train an BLSTM, there are several changes in the code that you should be aware of.

1. In DNN training, all frames (samples) are processed independently, so the frames in the training data are randomized across all utterances.
In RNN training, the network is trying to learn temporal patterns in the speech sequence, so the order of the utterances can be randomized but the utterances themselves must be kept intact. Thus, we set **frame_mode=False** in the **MinibatchSource** instantiated by **create_mb_source()**.

2. Change the network creation to create a BLSTM

  * In **create_network()** , we've created a function called **MyBLSTMLayer** as specified below.
This function uses the [Optimized_RNN Stack](https://docs.microsoft.com/en-us/cognitive-toolkit/OptimizedRNNStack) functionality in CNTK.
A complete description and additional examples can be found in the CNTK documentation.
One thing to be aware of is that with a BLSTM, the size of the hidden layer is actually applied to both directions.
Thus, setting the number of hidden units to 512 means that both the forward and backward layers consist of 512 cells.
The outputs of the forward and backward layer are then concatenated forming an output of 1024 units.
This is then projected back to 512 using the weight matrix W.
**def MyBLSTMLayer(hidden_size=128, num_layers=2): W = C.Parameter((C.InferredDimension, hidden_size), init=C.he_normal(1.0), name='rnn_parameters') def _func(operand): return C.optimized_rnnstack(operand, weights=W, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, recurrent_op='lstm') return _func**
  * The code calls **MyBLSTMLayer** when the **model_type** is **BLSTM**.
We've reduced the number of hidden layers to 2, since the BLSTM layers have more total parameters than the DNN layers.
  * For utterance based processing, entire utterance needs to be processed during training.
Thus the minibatch size specifies the total number of frames to process but will pack multiple utterances together if possible.
Setting the minibatch size to a larger number will allow for efficient processing with multiple utterances in each minibatch size.
We have set the minibatch size to 4096.

The traing the BLSTM model, you can execute the following command.

**$ python M3_Train_AM.py –-type BLSTM**

Because of the sequential nature of the BLSTM processing, they are inherently less parallelizable, and thus, train much slower than DNNs.
On a GTX 965M GPU running on a laptop, the network trained as a rate of 440 seconds per epoch, or 20 times slower than the DNN.
Thus, we will only train for 10 epochs to keep processing time reasonable.

Here too, you can use **M3_Plot_Training.py** to inspect the learning schedule in training.
And again, if you are interested, you can vary the hyperparameters to try to find a better solution.
